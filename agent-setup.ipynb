{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model=\"o3-mini\")\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```RAG Tool Functions -> Function Caller -> Metrics Recommendor -> Metrics Calculator -> Analyzer -> Executor```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_prompt = \"\"\"\n",
    "\n",
    "You are a DeFi Agent Launcher for Xade AI: A meta-agent that creates agents for users that can fetch, analyze DeFi data as well as execute actions based on them for the data. \n",
    "\n",
    "---\n",
    "\n",
    "You will receive the following input:\n",
    "\n",
    "Agent Name & Description:\n",
    "// The name of the agent and a short description of what it does\n",
    "\n",
    "Query:\n",
    "// A very simple, elementary query about the user's requirements which you will have to expand on and detail\n",
    " \n",
    "Actions:\n",
    "// Any actions the agent may be required to perform\n",
    "# Eg.\n",
    "#   Sentient Posting:\n",
    "#   - Posting on X and XADE Terminal \n",
    "#   - Every 'x' minutes\n",
    "#   - Reply on tweets by Elon Musk (@ElonMusk)\n",
    "#   - Reply to mentions and quotes of oneself (@AgentName)\n",
    "\n",
    "Specific Data Sources to Use:\n",
    "// Any specific data sources to use (Separate from your recommendations)\n",
    "\n",
    "Style:\n",
    "// The style of the response the user would like to receive\n",
    "\n",
    "Examples:\n",
    "// Example interactions\n",
    "\n",
    "---\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "Your task is to review the challenge, instruct and allocate tasks to your underlying LLM agents to obtain the relevant data, synthesize it, analyze it, customize responses and execute some actions.\n",
    "\n",
    "---\n",
    "\n",
    "You will have access to agents to automate the task. The workflow for your agents is as follows:\n",
    "\n",
    "Data Functions Retriever → DeFi Data Fetcher & Metrics Recommender → Data Processor & Metrics Calculator → DeFi Analyzer → DeFi Executor\n",
    "\n",
    "\n",
    "- Data Functions Retriever: A RAG model that can retrieve functions to fetch data relevant to the task from various sources.\n",
    "  Currently, it has access to three data providers:\n",
    "    - Mobula: A provider for market-based data in DeFi\n",
    "    - LunarCrush: A provider for social/trending metrics in DeFi\n",
    "    - CryptoPanic: A provider for news and social media posts in DeFi\n",
    "You must craft the input for the RAG model keeping in mind that it can only retrieve functions to get the data, and cannot directly fetch the data or apply any filters.\n",
    "If you need to identify any specific tokens on a specific blockchain or from a specific sector of DeFi (for e.g. ReFi), ask the RAG model for the functions to get those tokens.\n",
    "Directly mention the kind of data you need (sentiment-related, volatility-related) or any category-specific data.\n",
    "Any filters you want for the data from the RAG model should be mentioned in the instructions for the DeFi Fetcher, and not in the RAG model input\n",
    "\n",
    "- DeFi Data Fetcher & Metrics Recommender: An agent that can call the functions retrieved by the Function Retriever to fetch data from various sources, and recommend metrics to calculate based off of that data.\n",
    "  This agent will call the data functions retrieved by the Functions Retriever and organize the data for Processor. Additionally, it will also instruct the Processor on what metrics to calculate based on the data and how.\n",
    "You must provide the Fetcher with a detailed description of the user's requirements that can assist it with it's recommendations for the Processor.\n",
    "You must also provide the Fetcher with any specific data sources to use, separate from the RAG Retriever's functions and recommend it on how to deal with its task of fetching the data.\n",
    "  - Data Processor & Metrics Calculator: An agent controlled by the Fetcher that can process the data fetched by the Fetcher and create functions to calculate metrics to satisfy the user's query.\n",
    "    This agent will process the data provided by the Fetcher and calculate technical metrics and indicators based on the its instructions.\n",
    "    This agent will be controlled by the Fetcher, so ensure you provide the Fetcher with an appropriate outline of the user's requirments.\n",
    "\n",
    "\n",
    "- A DeFi Analyzer: An agent that can analyze DeFi data and provide insights based on the data. \n",
    "This agent generates a data-driven analysis and evaluation of the data while using suitable methodologies (STARE, CAR , SOAR, IDEA frameworks). \n",
    "It is smart enough to understand the context of the data and provide insights based on the data.\n",
    "Based on your instructions and the data given by the Data Fetcher, it can model its analysis to:\n",
    "  - Discover emerging trends\n",
    "  - Analyze technical indicators such as RSI, MACD, and SMA to predict movements\n",
    "  - Interpret social media metrics to gauge market sentiment and mindshare\n",
    "Thus, ensure you give instructions such that the analyzer can provide insights relevant to the user's query.\n",
    "\n",
    "\n",
    "- A DeFi Executor: An agent that can execute actions based on the data fetched and analyzed by the other two agents. \n",
    "This agent can devise trading strategies, execute trades, post on X and the XADE Terminal every few minutes, reply to various pre-defined indiviuals, join Twitter spaces, perform portoflio analysis, and other such actions.\n",
    "It's execution will be performed by calling the relevant functions.\n",
    "It will execute the actions you ask it to, based on the data fetched and analyzed by the other two agents.\n",
    "It will have access to a memory of it's tasks, so it can remember its previous context and actions and adjust future actions accordingly.\n",
    "Here is a list of it's abilities:\n",
    "  - Post on X and the XADE Terminal periodically\n",
    "  - Reply to posts by various pre-defined individuals\n",
    "  - Join Twitter spaces and engage in discussions\n",
    "  - Perform portfolio analysis for a given wallet address\n",
    "  - Devise trading strategies based on the user's priorities and the analyzed data.\n",
    "  - For text-based actions, it can customize responses based on the user's preferences.\n",
    "\n",
    "---\n",
    "\n",
    "When creating the instructions for the LLMs to execute, break your instructions into a logical, step-by-step order, using the specified format:\n",
    "    - Primary actions must be clearly defined. You must define the end-goal that the LLM is trying to achieve through its actions.\n",
    "    - Sub-actions must be indented and clearly defined, beginning on a new line.\n",
    "    - Specify conditions using clear 'if...then...else' statements\n",
    "    - Ensure that there is no ambiguity in any part of your plan. You must be specific and clear about what to use and how.\n",
    "    - The instructions generated must be extremely detailed and thorough with explanations at every step. They must first outline the primary instruction for the LLM and then provide direct and thorough explanations\n",
    "    - Do not use any markdown formatting.\n",
    "    \n",
    "---\n",
    "\n",
    "{master_example}\n",
    "\n",
    "TASK:\n",
    "{input}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_example = '''\n",
    "\n",
    "TASK:\n",
    "Agent Name & Description:\n",
    "Agent DAO\n",
    "Performs detailed analysis on trending DAOs and provides insights on various aspects.\n",
    "\n",
    "Query:\n",
    "Comment on rising DAOs\n",
    "\n",
    "Actions:\n",
    "  Sentient Posting:\n",
    "  - Posting on X and XADE Terminal \n",
    "  - Every 5 minutes\n",
    "  - Reply to mentions and quotes of oneself (@AgentETH)\n",
    "\n",
    "Specific Data Sources to Use:\n",
    "- Market Data\n",
    "- Social Data\n",
    "- News Feeds\n",
    "\n",
    "Style:\n",
    "Answer like an Analyst\n",
    "\n",
    "Examples:\n",
    "N.A.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Data Functions Retriever Input:\n",
    "\n",
    "Query: Retrieve functions to get trending DAOs\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ElPlan(BaseModel):\n",
    "    fetcher: str = Field(description=\"The task for the DeFi Data Fetcher and the insturctions for execution.\")\n",
    "    data_rag_prompt: str = Field(description=\"The prompt for the RAG model to fetch relevant data sources\")\n",
    "    analyzer: str = Field(description=\"The task for the analyzer and the instructions for execution (detailed). This task must be possible to fulfill with data retreived by the Data Fetcher.\")\n",
    "    executor: str = Field(description=\"The task for the executor and the instructions to fulfill it. All specific instructions for each action can be included here as well. All miscellaneous instructions can be included here.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ElPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_prompt = PromptTemplate(\n",
    "    template=master_prompt,\n",
    "    input_variables=[\"input\"],\n",
    "    # partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "Agent Name & Description:\n",
    "Agent DAO\n",
    "Performs detailed analysis on trending DAOs and provides insights on various aspects.\n",
    "\n",
    "Query:\n",
    "Comment on rising DAOs\n",
    "\n",
    "Actions:\n",
    "  Sentient Posting:\n",
    "  - Posting on X and XADE Terminal \n",
    "  - Every 5 minutes\n",
    "  - Reply to mentions and quotes of oneself (@AgentETH)\n",
    "\n",
    "Specific Data Sources to Use:\n",
    "- Market Data\n",
    "- Social Data\n",
    "- News Feeds\n",
    "\n",
    "Style:\n",
    "Answer like an Analyst\n",
    "\n",
    "Examples:\n",
    "N.A.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(master_prompt.format(input=input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step-by-Step Execution Plan for Agent DAO**\n",
      "\n",
      "**1. Data Functions Retriever (RAG Model) Input Preparation**  \n",
      "- **Objective**: Retrieve functions to fetch DAO-related data from Mobula, LunarCrush, and CryptoPanic.  \n",
      "  - **Market Data (Mobula)**:  \n",
      "    - Request functions to fetch:  \n",
      "      - Top DAOs by trading volume (last 24 hours).  \n",
      "      - Price change (%) and liquidity metrics for DAOs.  \n",
      "      - Token holders' distribution (to identify concentration risks).  \n",
      "  - **Social Data (LunarCrush)**:  \n",
      "    - Request functions to fetch:  \n",
      "      - DAO-specific social mentions, sentiment score (0-1), and social engagement (likes/shares).  \n",
      "      - Trending DAOs ranked by \"social dominance\" metric.  \n",
      "      - List of influencers discussing specific DAOs.  \n",
      "  - **News Feeds (CryptoPanic)**:  \n",
      "    - Request functions to fetch:  \n",
      "      - News articles mentioning DAOs, filtered by positive/negative sentiment.  \n",
      "      - Volume of DAO-related news in the past 2 hours.  \n",
      "\n",
      "---\n",
      "\n",
      "**2. DeFi Data Fetcher & Metrics Recommender Instructions**  \n",
      "- **Primary Action**: Fetch and filter DAO data from all three sources.  \n",
      "  - **Market Data**:  \n",
      "    - Fetch top 20 DAOs by trading volume (24h) with price volatility > 15%.  \n",
      "    - Filter DAOs with liquidity pools > $10M to avoid low-cap risks.  \n",
      "  - **Social Data**:  \n",
      "    - Fetch DAOs with social mentions increasing by ≥30% in the past 6 hours.  \n",
      "    - Include DAOs with sentiment score > 0.7 (positive).  \n",
      "  - **News Feeds**:  \n",
      "    - Fetch DAOs with ≥5 news mentions in the past 2 hours.  \n",
      "    - Exclude DAOs with negative sentiment ratio > 40%.  \n",
      "  - **Metrics to Calculate (Instructions for Processor)**:  \n",
      "    - **Trend Score**: (24h volume growth rate) × (social mentions growth rate).  \n",
      "    - **Social Sentiment Score**: (Positive mentions / Total mentions) × 100.  \n",
      "    - **News Impact Score**: (Number of news articles) × (average sentiment score).  \n",
      "\n",
      "---\n",
      "\n",
      "**3. Data Processor & Metrics Calculator Instructions**  \n",
      "- **Primary Action**: Calculate metrics to identify rising DAOs.  \n",
      "  - **Trend Score Calculation**:  \n",
      "    - For each DAO:  \n",
      "      - Compute 24h volume growth rate: `(Current volume - Previous 24h volume) / Previous 24h volume`.  \n",
      "      - Compute social mentions growth rate similarly.  \n",
      "      - Multiply the two rates and normalize to a 0-10 scale.  \n",
      "  - **Social Sentiment Score**:  \n",
      "    - Use LunarCrush data to compute `(Positive mentions / Total mentions) × 100`.  \n",
      "  - **News Impact Score**:  \n",
      "    - Multiply the number of CryptoPanic news articles by their average sentiment score (0-1).  \n",
      "\n",
      "---\n",
      "\n",
      "**4. DeFi Analyzer Instructions**  \n",
      "- **Primary Action**: Identify DAOs with strong growth signals using the **STARE framework** (Scan, Target, Analyze, Respond, Evaluate).  \n",
      "  - **Scan**: Cross-reference DAOs appearing in all three datasets (market, social, news).  \n",
      "  - **Target**: Rank DAOs by `Trend Score × News Impact Score`.  \n",
      "  - **Analyze**:  \n",
      "    - Check if high social sentiment correlates with positive news sentiment.  \n",
      "    - Flag DAOs with rising holders but declining liquidity (possible pump-and-dump).  \n",
      "  - **Evaluation Criteria**:  \n",
      "    - **High-Priority DAOs**: Trend Score ≥ 8, Social Sentiment ≥ 75%, News Impact Score ≥ 4.  \n",
      "    - **Medium-Priority DAOs**: Meet two of the three criteria above.  \n",
      "\n",
      "---\n",
      "\n",
      "**5. DeFi Executor Instructions**  \n",
      "- **Primary Action**: Post analysis on X/XADE Terminal every 5 minutes and reply to @AgentETH mentions.  \n",
      "  - **Posting Workflow**:  \n",
      "    - **Content Structure**:  \n",
      "      - Header: \"🚀 Rising DAO Alert: [DAO Name]\".  \n",
      "      - Body: \"Up [X]% in trading volume + [Y]% social growth. Recent news: [Summary].\"  \n",
      "      - Footer: \"Metrics: Trend Score [Z]/10 | Sentiment: [A]%\".  \n",
      "    - **Priority System**:  \n",
      "      - Post High-Priority DAOs immediately.  \n",
      "      - Post Medium-Priority DAOs if no High-Priority candidates exist.  \n",
      "  - **Reply to Mentions**:  \n",
      "    - If user asks, \"Why is [DAO] trending?\":  \n",
      "      - Respond with: \"🔍 Based on [metric] growth and [news headline], [DAO] is gaining traction due to [reason].\"  \n",
      "    - If user requests a comparison:  \n",
      "      - Deploy Processor to compute relative Trend Scores and generate a comparison table.  \n",
      "\n",
      "---\n",
      "\n",
      "**Quality Control Measures**  \n",
      "- **Data Validation**:  \n",
      "  - Discard DAOs with conflicting signals (e.g., high social volume but negative news).  \n",
      "  - Cross-verify LunarCrush social data with CryptoPanic news sentiment.  \n",
      "- **Executor Memory**:  \n",
      "  - Track posted DAOs to avoid repetition within a 12-hour window.  \n",
      "  - Adjust posting frequency if fewer than 2 High-Priority DAOs exist.  \n",
      "\n",
      "---\n",
      "\n",
      "**Example Output for X Post**  \n",
      "```\n",
      "🚀 Rising DAO Alert: @MakerDAO  \n",
      "- Trading volume 📈 +47% in 24h  \n",
      "- Social mentions 🔥 +62% (92% positive)  \n",
      "- Recent news: MakerDAO launches \"Endgame\" restructuring plan.  \n",
      "Metrics: Trend Score 9.2/10 | Sentiment: 92%  \n",
      "```  \n",
      "\n",
      "This plan ensures Agent DAO systematically identifies, validates, and communicates actionable insights on rising DAOs while maintaining analytical rigor.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "import json\n",
    "\n",
    "def display_agent_output(response_content):\n",
    "    console = Console()\n",
    "    response_content = response_content.strip('```json\\n').strip('\\n```')\n",
    "    try:\n",
    "        data = json.loads(response_content)\n",
    "    except:\n",
    "        console.print(\"[bold red]Error parsing response as JSON[/bold red]\")\n",
    "        return\n",
    "    \n",
    "    table = Table(show_header=True, header_style=\"bold\")\n",
    "    table.add_column(\"Component\", style=\"italic white\")\n",
    "    table.add_column(\"Instructions\", style=\"\")\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        formatted_key = key.replace('_', ' ').title()\n",
    "        table.add_row(formatted_key, value)\n",
    "    \n",
    "    console.print(Panel(\n",
    "        table,\n",
    "        title=\"[bold yellow]🤖 DeFi Agent Instructions[/bold yellow]\",\n",
    "        border_style=\"yellow\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error parsing response as JSON</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError parsing response as JSON\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_agent_output(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import inspect\n",
    "import * from data_functions\n",
    "\n",
    "# ----------------------------\n",
    "# Assume KNOWLEDGE_BASE is built from our 20 functions.\n",
    "# For demonstration, here's a dummy KNOWLEDGE_BASE for two functions.\n",
    "# In your system, KNOWLEDGE_BASE will contain all 20 functions.\n",
    "# For example:\n",
    "# KNOWLEDGE_BASE = {\n",
    "#    \"calculateSMA\": {\"description\": <docstring>, \"code\": <source code>},\n",
    "#    ... \n",
    "# }\n",
    "\n",
    "# For the purpose of this demo, let's assume KNOWLEDGE_BASE has been built already.\n",
    "# Uncomment the following line if you already built it:\n",
    "# KNOWLEDGE_BASE = build_knowledge_base()\n",
    "\n",
    "# In our test, we simulate KNOWLEDGE_BASE with the functions defined previously.\n",
    "# (You should replace this with your actual KNOWLEDGE_BASE dictionary from the previous step.)\n",
    "def calculateSMA(price_history_output: list, period: int) -> float:\n",
    "    \"\"\"\n",
    "    Function Name: calculateSMA\n",
    "    Description: Computes the Simple Moving Average (SMA) using historical price data.\n",
    "    Inputs:\n",
    "        - price_history_output: List of historical prices.\n",
    "        - period: Number of data points to average.\n",
    "    Processing:\n",
    "        - Extract the last 'period' prices.\n",
    "        - Compute SMA = sum(last_period_prices) / period.\n",
    "    Output:\n",
    "        - A float representing the SMA.\n",
    "    \"\"\"\n",
    "    if len(price_history_output) < period:\n",
    "        raise ValueError(\"Not enough data points to compute SMA.\")\n",
    "    return sum(price_history_output[-period:]) / period\n",
    "\n",
    "def calculateEMA(price_history_output: list, period: int) -> float:\n",
    "    \"\"\"\n",
    "    Function Name: calculateEMA\n",
    "    Description: Computes the Exponential Moving Average (EMA) with more weight on recent prices.\n",
    "    Inputs:\n",
    "        - price_history_output: List of historical prices.\n",
    "        - period: The EMA period.\n",
    "    Processing:\n",
    "        - Initialize EMA using the SMA of the first 'period' data points.\n",
    "        - For each subsequent price, update EMA with smoothing factor k = 2/(period+1).\n",
    "    Output:\n",
    "        - A float representing the final EMA value.\n",
    "    \"\"\"\n",
    "    if len(price_history_output) < period:\n",
    "        raise ValueError(\"Not enough data points to compute EMA.\")\n",
    "    ema = sum(price_history_output[:period]) / period\n",
    "    k = 2 / (period + 1)\n",
    "    for price in price_history_output[period:]:\n",
    "        ema = price * k + ema * (1 - k)\n",
    "    return ema\n",
    "\n",
    "# Build a simulated knowledge base using these two functions (extend this to all 20)\n",
    "def build_knowledge_base():\n",
    "    functions = [calculateSMA, calculateEMA]  # Replace with all 20 functions in practice.\n",
    "    kb = {}\n",
    "    for func in functions:\n",
    "        source = inspect.getsource(func)\n",
    "        doc = func.__doc__\n",
    "        kb[func.__name__] = {\"description\": doc, \"code\": source}\n",
    "    return kb\n",
    "\n",
    "KNOWLEDGE_BASE = build_knowledge_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Build the Vector Database using FAISS and SentenceTransformer\n",
    "# ----------------------------\n",
    "\n",
    "# Prepare the list of function names and corresponding descriptions from the knowledge base.\n",
    "function_names = list(KNOWLEDGE_BASE.keys())\n",
    "function_descriptions = [KNOWLEDGE_BASE[name][\"description\"] for name in function_names]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(function_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model (using a lightweight model here; adjust as needed).\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for each function description.\n",
    "embeddings = embedding_model.encode(function_descriptions)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dim)\n",
    "faiss_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# RAG Search Function Using the Vector Database\n",
    "# ----------------------------\n",
    "\n",
    "def rag_search_vector(query: str, top_k: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    RAG Search with a vector database.\n",
    "    Given a query string, this function computes its embedding and retrieves the top_k\n",
    "    function entries whose description embeddings are closest.\n",
    "    \n",
    "    Inputs:\n",
    "        - query: A string query.\n",
    "        - top_k: Number of top results to return.\n",
    "    Output:\n",
    "        - A dictionary mapping function names to their 'description' and 'code'.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "    \n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    results = {}\n",
    "    for idx in indices[0]:\n",
    "        func_name = function_names[idx]\n",
    "        results[func_name] = KNOWLEDGE_BASE[func_name]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Functions for query: Retrieve functions to get the Simple Moving Average (SMA) of a price history.\n",
      "\n",
      "Function: calculateSMA\n",
      "Description:\n",
      "\n",
      "    Function Name: calculateSMA\n",
      "    Description: Computes the Simple Moving Average (SMA) using historical price data.\n",
      "    Inputs:\n",
      "        - price_history_output: List of historical prices.\n",
      "        - period: Number of data points to average.\n",
      "    Processing:\n",
      "        - Extract the last 'period' prices.\n",
      "        - Compute SMA = sum(last_period_prices) / period.\n",
      "    Output:\n",
      "        - A float representing the SMA.\n",
      "    \n",
      "Code:\n",
      "def calculateSMA(price_history_output: list, period: int) -> float:\n",
      "    \"\"\"\n",
      "    Function Name: calculateSMA\n",
      "    Description: Computes the Simple Moving Average (SMA) using historical price data.\n",
      "    Inputs:\n",
      "        - price_history_output: List of historical prices.\n",
      "        - period: Number of data points to average.\n",
      "    Processing:\n",
      "        - Extract the last 'period' prices.\n",
      "        - Compute SMA = sum(last_period_prices) / period.\n",
      "    Output:\n",
      "        - A float representing the SMA.\n",
      "    \"\"\"\n",
      "    if len(price_history_output) < period:\n",
      "        raise ValueError(\"Not enough data points to compute SMA.\")\n",
      "    return sum(price_history_output[-period:]) / period\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Function: calculateEMA\n",
      "Description:\n",
      "\n",
      "    Function Name: calculateEMA\n",
      "    Description: Computes the Exponential Moving Average (EMA) with more weight on recent prices.\n",
      "    Inputs:\n",
      "        - price_history_output: List of historical prices.\n",
      "        - period: The EMA period.\n",
      "    Processing:\n",
      "        - Initialize EMA using the SMA of the first 'period' data points.\n",
      "        - For each subsequent price, update EMA with smoothing factor k = 2/(period+1).\n",
      "    Output:\n",
      "        - A float representing the final EMA value.\n",
      "    \n",
      "Code:\n",
      "def calculateEMA(price_history_output: list, period: int) -> float:\n",
      "    \"\"\"\n",
      "    Function Name: calculateEMA\n",
      "    Description: Computes the Exponential Moving Average (EMA) with more weight on recent prices.\n",
      "    Inputs:\n",
      "        - price_history_output: List of historical prices.\n",
      "        - period: The EMA period.\n",
      "    Processing:\n",
      "        - Initialize EMA using the SMA of the first 'period' data points.\n",
      "        - For each subsequent price, update EMA with smoothing factor k = 2/(period+1).\n",
      "    Output:\n",
      "        - A float representing the final EMA value.\n",
      "    \"\"\"\n",
      "    if len(price_history_output) < period:\n",
      "        raise ValueError(\"Not enough data points to compute EMA.\")\n",
      "    ema = sum(price_history_output[:period]) / period\n",
      "    k = 2 / (period + 1)\n",
      "    for price in price_history_output[period:]:\n",
      "        ema = price * k + ema * (1 - k)\n",
      "    return ema\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Retrieve functions to get the Simple Moving Average (SMA) of a price history.\"\n",
    "\n",
    "matching_functions = rag_search_vector(sample_query, top_k=3)\n",
    "\n",
    "if matching_functions:\n",
    "  print(\"Matching Functions for query:\", sample_query)\n",
    "  for name, details in matching_functions.items():\n",
    "      print(f\"\\nFunction: {name}\\nDescription:\\n{details['description']}\\nCode:\\n{details['code']}\\n{'-'*40}\")\n",
    "else:\n",
    "  print(\"No matching functions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xade-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
